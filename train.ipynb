{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"13rjXQpaLq3JqblPaHhzTnL7DP-MK-8oD","authorship_tag":"ABX9TyPQhnKLJymKUGY2FUaWpspQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["! unzip \"/content/drive/MyDrive/ECE3001_Project/stu_dataset.zip\" -d \"/content/drive/MyDrive/ECE3001_Project/\""],"metadata":{"id":"B_N_qvOrDx2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install timm"],"metadata":{"id":"0Zijp-XmQl_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"SupnSiJcoTe2","executionInfo":{"status":"ok","timestamp":1711026656751,"user_tz":-480,"elapsed":6724,"user":{"displayName":"谢悦皎","userId":"04926220452837867737"}}},"outputs":[],"source":["import torch\n","import argparse\n","from tqdm import tqdm\n","import datetime\n","import time\n","from timm.utils import accuracy\n","\n","import librosa\n","import numpy as np\n","import librosa.display\n","import math\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torch.nn as nn\n","from torchvision.models import vgg11, vgg11_bn, vgg13\n","from torchvision.models import resnet18"]},{"cell_type":"code","source":["# Loading data\n","class AudioDataset(Dataset):\n","    def __init__(self, data_dir, max_len, window_length, window_shift, use_stft):\n","        self.data_dir = data_dir\n","        self.file_list = os.listdir(data_dir)\n","        self.max_len = max_len\n","        self.window_shift = window_shift\n","        self.window_length = window_length\n","        self.use_stft = use_stft\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        filename = os.path.join(self.data_dir, self.file_list[idx])\n","        wav_data = extract_hpss_features_sg(filename, max_length=self.max_len, window_length=self.window_length, window_shift=self.window_shift, use_stft=self.use_stft)\n","        wav_data = torch.tensor(wav_data)\n","        wav_data = wav_data.unsqueeze(0)\n","\n","        # Parse label from filename (filename format: id1_filename.wav)\n","        label = self.file_list[idx].split('_')[0][2:]  # Extract label from filename\n","        label = torch.tensor([int(label)])\n","\n","        return wav_data, label\n","\n","def extract_hpss_features_sg(wav_path, max_length, window_length=320, window_shift=160, use_stft=True):\n","    \"\"\"Extract Harmonic-Percussive Source Separation features.\n","\n","    Args:\n","      wav_dir: string, directory of wavs.\n","      out_dir: string, directory to write out features.\n","      recompute: bool, if True recompute all features, if False skip existed\n","                 extracted features.\n","    \"\"\"\n","    cnt = 0\n","    t1 = time.time()\n","    (audio, sr) = read_audio(wav_path)\n","\n","    if audio.shape[0] == 0:\n","        print(\"File %s is corrupted!\" % wav_path)\n","        raise ValueError\n","    else:\n","        # librosa.display.waveshow(audio, sr=sr)\n","        # plt.show()\n","\n","        if use_stft: # compute stft\n","            spec = np.log(get_spectrogram(audio, window_length, window_shift) + 1e-8)\n","        else: # not use stft\n","            frame = 256\n","            split_num = math.floor(audio.shape[0] / frame)\n","            new_audio = np.split(audio[:split_num*frame], split_num)\n","            spec = np.stack(new_audio, axis=0).T\n","\n","        spec = norm(spec)\n","        spec = spec.T\n","        spec = pad_trunc_seq(spec, max_length)\n","\n","        # cnt += 1\n","    # print(\"Thread %d Extracting feature time: %s\" % (i, (time.time() - t1)))\n","    return spec\n","\n","def read_audio(path, target_fs=None):\n","    try :\n","        audio, fs = librosa.load(path, sr=None) # fs:sample rate\n","    except:\n","        print(path)\n","\n","    if audio.ndim > 1:  # 维度>1，这里考虑双声道的情况，维度为2，在第二个维度上取均值，变成单声道\n","        audio = np.mean(audio, axis=1)\n","    if target_fs is not None and fs != target_fs:\n","        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)  # 重采样输入信号，到目标采样频率\n","        fs = target_fs\n","    return audio, fs\n","\n","def pad_trunc_seq(x, max_len):\n","    \"\"\"Pad or truncate a sequence data to a fixed length.\n","\n","    Args:\n","      x: ndarray, input sequence data.\n","      max_len: integer, length of sequence to be padded or truncated.\n","\n","    Returns:\n","      ndarray, Padded or truncated input sequence data.\n","    \"\"\"\n","    L = len(x)\n","    shape = x.shape\n","    if L < max_len:\n","        pad_shape = (max_len - L,) + shape[1:]\n","        pad = np.zeros(pad_shape)\n","        x_new = np.concatenate((x, pad), axis=0)\n","    else:\n","        x_new = x[0:max_len]\n","\n","    return x_new\n","\n","def get_spectrogram(wav, win_length, win_shift):\n","    D = librosa.stft(wav, n_fft=win_length, hop_length=win_shift, win_length=win_length, window='hamming')\n","    spect, phase = librosa.magphase(D)\n","    return spect\n","\n","\n","def norm(spec):\n","    mean = np.reshape(np.mean(spec, axis=1), (spec.shape[0],1))\n","    std = np.reshape(np.std(spec, axis=1), (spec.shape[0],1))\n","    spec = np.divide(np.subtract(spec,np.repeat(mean, spec.shape[1], axis=1)), np.repeat(std, spec.shape[1], axis=1))\n","    return spec\n"],"metadata":{"id":"XV9IBC90uZVs","executionInfo":{"status":"ok","timestamp":1711026660432,"user_tz":-480,"elapsed":324,"user":{"displayName":"谢悦皎","userId":"04926220452837867737"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## You can try different models in this part."],"metadata":{"id":"GdLPLBMZSx-Y"}},{"cell_type":"code","source":["# Loading Model\n","class vgg_base(nn.Module):\n","    def __init__(self, input_dim):\n","        super(vgg_base,self).__init__()\n","        self.vggmodel=vgg11(pretrained=False).features\n","        self.vggmodel[0]=nn.Conv2d(input_dim,64,kernel_size = 3, padding= 1)\n","\n","    def forward(self, x):\n","        x = self.vggmodel(x)\n","        return x\n","\n","class vggbn_base(nn.Module):\n","    def __init__(self, input_dim):\n","        super(vggbn_base,self).__init__()\n","        self.vggmodel=vgg11_bn(pretrained=False).features\n","        self.vggmodel[0]=nn.Conv2d(input_dim,64,kernel_size = 3, padding= 1)\n","\n","    def forward(self, x):\n","        x = self.vggmodel(x)\n","        return x\n","\n","\n","class resnet_base(nn.Module):\n","    def __init__(self, input_dim):\n","        super(resnet_base,self).__init__()\n","        self.resnetmodel=resnet18(pretrained=False)\n","        self.resnetmodel.conv1=nn.Conv2d(input_dim,64,kernel_size = 7, stride=2,padding= 3,bias=False)\n","\n","    def forward(self, x):\n","        x = self.resnetmodel(x)\n","        return x\n","\n","class My_model(nn.Module):\n","    def __init__(self, input_dim=1, num_classes=93, model_base=\"vgg\"):\n","        super(My_model,self).__init__()\n","        if model_base == \"vgg\":\n","            self.backbone=vgg_base(input_dim)\n","        elif model_base == \"vggbn\":\n","            self.backbone=vggbn_base(input_dim)\n","        elif model_base ==\"resnet\":\n","            self.backbone=resnet_base(input_dim)\n","\n","        self.model_base=model_base\n","        self.avgpool = nn.AvgPool1d(kernel_size=200, stride=1)\n","        self.linear = nn.Linear(in_features=512, out_features=num_classes)\n","        self.linear2 = nn.Linear(in_features=1000, out_features=num_classes)\n","        self.activate = nn.Softmax(dim=1)\n","        self.criteria = nn.CrossEntropyLoss()\n","\n","    def forward(self, input, label=None):\n","        result = self.backbone(input)\n","        if self.model_base in [\"vgg\",\"vggbn\"]:\n","            result = result.view(result.size(0), result.size(1), -1)\n","            result = self.avgpool(result)\n","            result = result.reshape(result.size(0), -1)\n","            result = self.linear(result)\n","\n","        elif self.model_base == \"resnet\":\n","            result = self.linear2(result)\n","\n","        result = self.activate(result)\n","\n","        _, pred_label = result.max(-1)\n","\n","        if label is not None: # train\n","            loss = self.criteria(result, label.view(-1))\n","            return loss, result, pred_label\n","        else: # test\n","            return result, pred_label\n"],"metadata":{"id":"eEPQoWtdvP8-","executionInfo":{"status":"ok","timestamp":1711026664624,"user_tz":-480,"elapsed":420,"user":{"displayName":"谢悦皎","userId":"04926220452837867737"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# train and valid\n","def valid(args, model):\n","    print('Predcting...')\n","    audio_testset = AudioDataset(args.test_path, args.max_len, args.window_length, args.window_shift, args.use_stft)\n","    test_data = DataLoader(audio_testset, batch_size=args.batchsize, shuffle=False)\n","\n","    model.eval()\n","    acc1_total = 0.\n","    acc5_total = 0.\n","    step = 0\n","\n","    with torch.no_grad():\n","        for step, (x, label) in enumerate(tqdm(test_data)):\n","            x = x.to(dtype=torch.float32, device=device)\n","            label = label.to(device)\n","            result, pred = model(x)\n","            acc1, acc5 = accuracy(result, label.view(-1), topk=(1, 5))\n","            acc1, acc5 = acc1.item()/100, acc5.item()/100\n","            # loss_total += float(loss.item())\n","            acc1_total += acc1\n","            acc5_total += acc5\n","    print(\"Valid_acc1:{}, Valid_acc5: {}\".format( acc1_total / (step+1), acc5_total / (step+1) ))\n","    return acc1_total / (step+1), acc5_total / (step+1)\n","\n","def train(args):\n","    model = My_model(num_classes=92, model_base=args.model_base)\n","\n","    # load pretrained model\n","    # model.load_state_dict(torch.load(args.load_model_path,map_location=device))\n","\n","    model = model.to(dtype=torch.float32, device=device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n","    best_epoch = -1\n","    best_acc1 = 0\n","    best_acc5 = 0\n","    best_model = 0\n","\n","    # valid(args, model)\n","    audio_trainset = AudioDataset(args.train_path, args.max_len, args.window_length, args.window_shift, args.use_stft)\n","    print(f\"Length of training set: {len(audio_trainset)}\")\n","    train_data = DataLoader(audio_trainset, batch_size=args.batchsize, shuffle=True, drop_last=True)\n","\n","    for epoch in range(args.epochs):\n","        start_time = time.time()\n","        model.train()\n","        acc1_total = 0.\n","        acc5_total = 0.\n","        loss_total = 0.\n","        for step, (x, label) in enumerate(tqdm(train_data)):\n","            x = x.to(dtype=torch.float32, device=device)\n","            label = label.to(device)\n","            optimizer.zero_grad()\n","            loss, result, pred = model(x, label)\n","            acc1, acc5 = accuracy(result, label.view(-1), topk=(1, 5))\n","            try:\n","                acc1, acc5 = acc1.item() / 100, acc5.item() / 100\n","            except:\n","                print(\"testt\")\n","            loss.backward()\n","            optimizer.step()\n","            acc1_total += acc1\n","            acc5_total += acc5\n","            loss_total += float(loss.item())\n","            if step % args.print_every == 0 and step != 0:\n","                print('epoch %d, step %d, step_loss %.4f, step_acc1 %.4f, step_acc5 %.4f' % (epoch, step, loss_total/(step+1), acc1_total/(step+1), acc5_total/(step+1)))\n","\n","        # save model\n","        # if epoch % args.save_every == 0 and epoch != 0:\n","        #     if args.use_stft:\n","        #         model_name = args.save_model_path+ \"_epoch_\"+ str(epoch)+'_stft.pt'\n","        #     else:\n","        #         model_name = args.save_model_path + \"_epoch_\"+ str(epoch) + 'no_stft.pt'\n","        #     torch.save(model.state_dict(), model_name)\n","        acc1, acc5 = valid(args, model)\n","        if acc1 > best_acc1:\n","            best_acc1 = acc1\n","            best_acc5 = acc5\n","            best_epoch = epoch\n","            best_model = model\n","            # torch.save(model.state_dict(), args.checkpoint_path+'_pretrain.pt')\n","        print('best acc1 is: {}, acc5 is: {}, in epoch {}'.format(best_acc1, best_acc5, best_epoch))\n","\n","        total_time = time.time() - start_time\n","        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","        print('Training time {}'.format(total_time_str))\n"],"metadata":{"id":"LBw5KbVhoYLV","executionInfo":{"status":"ok","timestamp":1711026668074,"user_tz":-480,"elapsed":305,"user":{"displayName":"谢悦皎","userId":"04926220452837867737"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Model Training"],"metadata":{"id":"cBKupd2sviEr"}},{"cell_type":"code","source":["# 修改工作路径\n","%cd /content/drive/MyDrive/ECE3001_Project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pycMSwvXNBNC","executionInfo":{"status":"ok","timestamp":1711027984398,"user_tz":-480,"elapsed":366,"user":{"displayName":"谢悦皎","userId":"04926220452837867737"}},"outputId":"b315ac4f-8abf-45b6-8167-fc8dfb9596a8"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ECE3001_Project\n"]}]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HDFjyJerNm-5","executionInfo":{"status":"ok","timestamp":1711027993159,"user_tz":-480,"elapsed":10,"user":{"displayName":"谢悦皎","userId":"04926220452837867737"}},"outputId":"4862add1-a3f5-4900-c35b-3b0dbaf6bd8a"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/ECE3001_Project'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["device = 'cuda:0' #'cpu'\n","device = torch.device(device)\n","# def parse_config():\n","parser = argparse.ArgumentParser()\n","# path parameters\n","parser.add_argument('--train_path', type=str, default='./stu_dataset/train')\n","parser.add_argument('--test_path', type=str, default='./stu_dataset/test')\n","parser.add_argument('--save_model_path', type=str, default='./model/vggbn/vggbn11')\n","# parser.add_argument('--load_model_path', type=str, default='./model/vggbn/vggbn11_epoch_19_stft.pt')\n","\n","# training parameters\n","parser.add_argument('--epochs', type=int, default=20)\n","parser.add_argument('--print_every', type=int, default=10)\n","parser.add_argument('--save_every', type=int, default=1)\n","parser.add_argument('--batchsize', type=int, default=64)\n","parser.add_argument('--lr', type=float, default=1e-4,help=\"learning rate\")\n","parser.add_argument('--model_base', type=str, default=\"vggbn\",help=\"model base: vgg, resnet, vggbn\")\n","\n","# data processing parameters\n","parser.add_argument(\"--max_len\", default=800, type=int, help=\"max_len\")\n","parser.add_argument(\"--window_shift\", default=256, type=int, help=\"hop shift\")\n","parser.add_argument(\"--window_length\", default=510, type=int, help=\"window length\") # 256\n","parser.add_argument(\"--use_stft\", default=True, type=bool, help=\"whether to use stft\")\n","# return parser.parse_args()\n","# parser = argparse.ArgumentParser()\n","# args = parser.parse_args(\n","#     [\"--train_path\",\"../stu_dataset/train\"],\n","#     [\"--test_path\",\"../stu_dataset/test\"],\n","#     [\"--save_model_path\",\"./model/vggbn/vggbn11\"],\n","#     [\"--epochs\", 20],\n","#     [\"--print_every\", 10],\n","#     [\"--save_every\", 10],\n","#     [\"--batchsize\", 32],\n","#     [\"--lr\", 1e-4],\n","#     [\"--model_base\", \"vggbn\"],\n","#     [\"--max_len\", 800],\n","#     [\"--window_shift\", 256],\n","#     [\"window_length\", 510],\n","#     [\"--use_stft\", True]\n","#                          )\n","args = parser.parse_args(args=[])\n","train(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNoyPUo8wAdb","outputId":"4e5f6afe-185d-4c88-df4f-f827ee87906e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of training set: 4769\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/74 [00:00<?, ?it/s]"]}]}]}